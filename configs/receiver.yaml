# Configuration for receiver prediction task

# Model configuration
# Same as overfit test to ensure consistency
model:
  input_dim: 16  # Node feature dimension: x, y, vx, vy, height, weight, dx_to_kicker, dy_to_kicker, dist_to_kicker, angle_to_kicker, dx_to_goal, dy_to_goal, dist_to_goal, angle_to_goal, ball, team
  hidden_dim: 128  # Same as overfit test
  num_classes: 22  # Number of players
  num_layers: 3  # Same as overfit test
  num_heads: 4  # Same as overfit test
  dropout: 0.0  # Same as overfit test (no regularization for debugging)

# Optimizer configuration
# Same as overfit test
optimizer:
  type: "adam"
  lr: 0.001  # Same as overfit test
  weight_decay: 0.0  # Same as overfit test (no weight decay for debugging)

# Scheduler configuration
# Same as overfit test (no scheduler)
scheduler:
  type: "none"  # Same as overfit test (fixed learning rate, no scheduler)

# Training configuration
# batch_size and epochs kept for full training, but amp disabled like overfit test
train:
  batch_size: 32  # Keep larger batch for full dataset (overfit test uses 4)
  epochs: 50  # Keep 50 epochs for full training
  amp: false  # Same as overfit test (disabled for stability)

# Data configuration
data:
  train_path: "data/processed_ck/receiver_train/data.pickle"
  val_path: "data/processed_ck/receiver_val/data.pickle"
  test_path: "data/processed_ck/receiver_test/data.pickle"
  format: "pickle"

# Loss configuration
# Same as overfit test (no regularization)
loss:
  type: "cross_entropy"
  label_smoothing: 0.0  # Same as overfit test (no label smoothing)
  weight: 1.0  # Loss weight
  
  # Focal loss parameters (alternative to cross-entropy)
  focal_loss:
    enabled: false
    alpha: 1.0  # Weighting factor for rare class
    gamma: 2.0  # Focusing parameter
    
  # Class weights for imbalanced datasets
  class_weights: null  # [w_0, w_1, ..., w_{num_classes-1}]
  
  # Regularization terms
  regularization:
    l2_weight: 0.0  # Same as overfit test (no L2 regularization)
    dropout_rate: 0.0  # Same as overfit test (no dropout regularization)

# D2 equivariance configuration
# Same as overfit test (no augmentation)
d2:
  group_pool: false  # Disable group pooling; ReceiverModel handles D2 internally
  transforms:
    hflip: false  # Same as overfit test (no augmentation)
    vflip: false  # Same as overfit test (no augmentation)

# Early stopping configuration
early_stopping:
  patience: 20  # Increased from 10 to 20 for larger model (more time to learn)
  min_delta: 0.005  # Reduced from 0.01 to 0.005 (more sensitive to small improvements)

# Evaluation configuration
eval:
  batch_size: 5
  min_cands_eval: 1

# System configuration
device: "auto"  # auto, cpu, cuda, mps
seed: 42
num_workers: 4  # Reduced for memory efficiency (was 12)
prefetch_factor: 2  # Reduced for memory efficiency (was 4)
persistent_workers: true  # Keep workers alive between epochs (reduces overhead)
log_level: "INFO"
log_dir: "runs"
checkpoint_dir: "checkpoints"
