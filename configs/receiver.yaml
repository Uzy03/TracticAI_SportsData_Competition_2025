# Configuration for receiver prediction task

# Model configuration
model:
  input_dim: 16  # Node feature dimension: x, y, vx, vy, height, weight, dx_to_kicker, dy_to_kicker, dist_to_kicker, angle_to_kicker, dx_to_goal, dy_to_goal, dist_to_goal, angle_to_goal, ball, team
  hidden_dim: 256  # Increased from 128 to 256 for better model capacity
  num_classes: 22  # Number of players
  num_layers: 4  # Increased from 3 to 4 for deeper model
  num_heads: 8  # Increased from 4 to 8 for more attention heads
  dropout: 0.15  # Reduced from 0.2 for larger model (less regularization)
  mlp_num_layers: 3  # MLP head layers (increased from default 2 to 3)

# Optimizer configuration
optimizer:
  type: "adam"
  lr: 0.001  # Reduced from 0.003 to prevent loss explosion (step 1: learning rate adjustment)
  weight_decay: 0.00005  # Reduced from 0.0001 for larger model

# Scheduler configuration
# StepLR: Reduce learning rate by gamma every step_size epochs
# lr=0.001 -> 0.0005 (epoch 10) -> 0.00025 (epoch 20) -> 0.000125 (epoch 30) -> ...
scheduler:
  type: "step"
  step_size: 10  # Reduce LR every 10 epochs
  gamma: 0.5  # Multiply LR by 0.5 at each step

# Training configuration
train:
  batch_size: 32  # Restored to original (memory-efficient implementation allows larger batch)
  epochs: 50  # ~18秒/エポック × 50 = ~15分（余裕を持たせて50エポック）
  amp: true  # Automatic mixed precision

# Data configuration
data:
  train_path: "data/processed_ck/receiver_train/data.pickle"
  val_path: "data/processed_ck/receiver_val/data.pickle"
  test_path: "data/processed_ck/receiver_test/data.pickle"
  format: "pickle"

# Loss configuration
loss:
  # Cross-entropy loss with label smoothing
  # L = -Σ y_i * log(p_i) where p_i = softmax(logits_i)
  # With label smoothing: y_i = (1 - α) * y_i + α / num_classes
  type: "cross_entropy"
  label_smoothing: 0.05  # Reduced from 0.1 for larger model (less regularization)
  weight: 1.0  # Loss weight
  
  # Focal loss parameters (alternative to cross-entropy)
  focal_loss:
    enabled: false
    alpha: 1.0  # Weighting factor for rare class
    gamma: 2.0  # Focusing parameter
    
  # Class weights for imbalanced datasets
  class_weights: null  # [w_0, w_1, ..., w_{num_classes-1}]
  
  # Regularization terms
  regularization:
    l2_weight: 0.0001  # L2 regularization weight
    dropout_rate: 0.2  # Dropout rate

# D2 equivariance configuration
d2:
  group_pool: false  # Disable group pooling; ReceiverModel handles D2 internally
  transforms:
    hflip: true
    vflip: true

# Early stopping configuration
early_stopping:
  patience: 20  # Increased from 10 to 20 for larger model (more time to learn)
  min_delta: 0.005  # Reduced from 0.01 to 0.005 (more sensitive to small improvements)

# Evaluation configuration
eval:
  batch_size: 5
  min_cands_eval: 1

# System configuration
device: "auto"  # auto, cpu, cuda, mps
seed: 42
num_workers: 4  # Reduced for memory efficiency (was 12)
prefetch_factor: 2  # Reduced for memory efficiency (was 4)
persistent_workers: true  # Keep workers alive between epochs (reduces overhead)
log_level: "INFO"
log_dir: "runs"
checkpoint_dir: "checkpoints"
